import streamlit as st
import os
import sys
import textwrap
from google import genai
from google.genai import types
from pypdf import PdfReader
from numpy.linalg import norm
from numpy import dot
import numpy as np
from dotenv import load_dotenv


def batch_list(iterable, n=100):
    """Bir listeyi N boyutunda par癟alara (batch) ay覺r覺r."""
    l = len(iterable)
    for ndx in range(0, l, n):
        yield iterable[ndx:min(ndx + n, l)]


# --- Gemini RAG Fonksiyonlar覺 ---

def chunk_text(text, chunk_size=1000, overlap_ratio=0.2):
    overlap = int(chunk_size * overlap_ratio)
    chunks = []
    start = 0
    while start < len(text):
        end = start + chunk_size
        chunk = text[start:end]
        chunks.append(chunk)
        start += (chunk_size - overlap)
        if chunk_size - overlap <= 0:
            break
    return chunks


@st.cache_resource
def setup_rag_pipeline():
    """
    RAG pipeline'覺n覺 kurar, PDF'leri y羹kler ve vekt繹rletirir.
    """

    load_dotenv()
    api_key_value = os.environ.get('GEMINI_API_KEY')

    if not api_key_value:
        st.error("API Anahtar覺 bulunamad覺. L羹tfen run_app.py dosyas覺ndaki API_KEY'i kontrol edin.")
        return None, None, None

    try:
        client = genai.Client(api_key=api_key_value)
        embedding_model = 'text-embedding-004'
        file_paths = ["5746_Kanun.pdf", "Arge_Yonetmelik.pdf"]
        raw_text = ""

        # 1. Dosya Y羹kleme
        for file_path in file_paths:
            try:
                reader = PdfReader(file_path)
                for page in reader.pages:
                    raw_text += page.extract_text()
            except FileNotFoundError:
                st.error(
                    f"Kritik Hata: '{file_path}' bulunamad覺. L羹tfen dosyan覺n proje dizininde olduundan emin olun.")
                return None, None, None

        # 2. Par癟alama
        text_chunks = chunk_text(raw_text, chunk_size=1000, overlap_ratio=0.2)
        if not text_chunks:
            st.error("Par癟alama baar覺s覺z. PDF dosyalar覺n覺 kontrol edin.")
            return None, None, None

        # 3. Vekt繹rleme (Embeddings)
        st.info(f"Toplam {len(text_chunks)} par癟a vekt繹rletiriliyor... (Batch boyutu: 100)")

        chunk_embeddings = []
        for batch in batch_list(text_chunks, 100):
            embeddings = client.models.embed_content(
                model=embedding_model,
                contents=batch,
            ).embeddings
            chunk_embeddings.extend(embeddings)

        indexed_data = list(zip(text_chunks, [e for e in chunk_embeddings]))
        st.success(f"RAG Veri Seti baar覺yla oluturuldu: {len(indexed_data)} par癟a.")

        return client, indexed_data, embedding_model
    except Exception as e:
        st.error(f"RAG kurulumunda KR襤T襤K HATA olutu: {e}")
        st.warning("Bu genellikle yanl覺 API anahtar覺 (400 Invalid Argument) veya balant覺 sorunlar覺ndan kaynaklan覺r.")
        return None, None, None


def get_rag_response(client, query, indexed_data, embedding_model, chat_history, top_k=5):
    """Sorguyu al覺r, ilgili dok羹manlar覺 癟eker ve Gemini'ye g繹nderir."""

    # a. Sorguyu Vekt繹rleme (Query Embedding)
    query_embedding_response = client.models.embed_content(
        model=embedding_model,
        contents=[query],
    ).embeddings[0]

    # ContentEmbedding objesini np.array'e d繹n羹t羹r
    query_embedding = np.array(query_embedding_response)

    # b. En Yak覺n Komular覺 Bulma (Retrieval - Kosin羹s Benzerlii)
    scores = []
    for chunk_text, chunk_embed in indexed_data:
        # Vekt繹r veritaban覺 objesini np.array'e d繹n羹t羹r
        np_chunk_embed = np.array(chunk_embed)

        # Kosin羹s Benzerlii Hesaplamas覺
        try:
            score = dot(query_embedding, np_chunk_embed) / (norm(query_embedding) * norm(np_chunk_embed))
        except Exception as e:
            # Eer norm s覺f覺r ise (bo chunk), skoru s覺f覺r yap
            score = 0.0

        scores.append(score)

    top_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:top_k]

    context = "\n\n---\n\n".join([indexed_data[i][0] for i in top_indices])

    # c. Gemini Prompter (Generation)
    history_str = "\n".join([f"Kullan覺c覺: {h['user']}\nAsistan: {h['assistant']}" for h in chat_history])

    # *** GNCELLENM襤 S襤STEM TAL襤MATI (Kayna覺n Ad覺 Belirtiliyor) ***
    system_instruction = f"""Sen bir Ar-Ge mevzuat dan覺man覺s覺n. Cevap verirken KES襤NL襤KLE 繹ncelikle aa覺daki 'VER襤 KAYNAI' b繹l羹m羹ndeki bilgileri kullan.

    KURALLAR:
    1. Yaln覺zca kaynakta bilgi varsa, cevab覺 kaynaa g繹re ver.
    2. Kaynakta bilgi YOKSA, NCE 'Kaynak d繹k羹manlarda (5746 say覺l覺 Kanun / Arge Y繹netmelik) bu bilgiye dorudan ula覺lamamaktad覺r.' ifadesini kullan.
    3. Hemen ard覺ndan, bu konuyla ilgili k覺sa ve genel bir bilgi/yorum ekle. Cevab覺n 2-3 c羹mleyi ge癟mesin.

    L羹tfen her zaman kesin ve mevzuata uygun bir dil kullan.

    SOHBET GEM襤襤:
    {history_str}

    VER襤 KAYNAI (Mevzuattan Al覺nan Par癟alar):
    {context}
    """

    response = client.models.generate_content(
        model='gemini-2.5-flash',
        contents=[
            {"role": "user", "parts": [{"text": query}]}
        ],
        config=types.GenerateContentConfig(
            system_instruction=system_instruction
        )
    )

    sources = [{"page_content": indexed_data[i][0], "source": "Y羹kl羹 Mevzuat"} for i in top_indices]

    return response.text, sources


# --- Streamlit Uygulamas覺 Aray羹z羹 ---
st.set_page_config(page_title="Kurumsal Ar-Ge Mevzuat Chatbot", layout="wide")

# BALIK
st.title(" Ar-Ge Mevzuat Dan覺man覺")
st.caption("Veri kayna覺: Kurumsal Ar-Ge Mevzuatlar覺 (Gemini API ile dorudan entegrasyon).")

# Session state'i kullanarak RAG pipeline'覺n覺 bir kez kurma
if 'client' not in st.session_state:
    client, indexed_data, embedding_model = setup_rag_pipeline()
    st.session_state.client = client
    st.session_state.indexed_data = indexed_data
    st.session_state.embedding_model = embedding_model
    st.session_state.chat_history = []
    st.session_state.current_message = None

client = st.session_state.client
indexed_data = st.session_state.indexed_data
embedding_model = st.session_state.embedding_model

if not client or not indexed_data:
    st.stop()

# --- CHAT GEM襤襤N襤 GSTERME ---
for message in st.session_state.chat_history:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

        # Kaynaklar覺 ge癟mi mesajlarda g繹sterme
        if message["role"] == "assistant" and "sources" in message:
            with st.expander("Kullan覺lan Kaynaklar (RAG Kan覺t覺)"):
                for i, doc in enumerate(message["sources"], 1):
                    source_text = doc['page_content']
                    st.markdown(f"**Kaynak {i}:** **Dosya:** {doc.get('source', 'Y羹kl羹 Mevzuat')}")
                    st.code(source_text[:500] + ('...' if len(source_text) > 500 else ''), language='markdown')

# rnek Sorular
example_questions = [
    "5746 say覺l覺 kanunun amac覺 nedir?",
    "Ar-Ge faaliyetleri kapsam覺nda gelir vergisi stopaj覺 teviki nas覺l uygulan覺r?",
    "Ar-Ge indirimi hangi gider kalemlerini kapsar?"
]

# Giri ekran覺nda 繹rnek sorular覺 g繹sterme
if st.session_state.current_message is None and not st.session_state.chat_history:
    st.subheader("rnek Sorular (Test K覺lavuzu)")
    for q in example_questions:
        if st.button(q, use_container_width=True):
            st.session_state.current_message = q
            st.rerun()

# Kullan覺c覺 girii yakalama
if user_query := st.chat_input("Ar-Ge mevzuat覺 ile ilgili bir soru sorun..."):
    st.session_state.current_message = user_query

# Mesaj覺 襤leme ve Cevaplama
if st.session_state.current_message:
    user_query = st.session_state.current_message

    with st.chat_message("user"):
        st.markdown(user_query)

    with st.chat_message("assistant"):
        with st.spinner("Cevap aran覺yor..."):
            try:
                history_for_rag = [
                    {"user": h["content"], "assistant": h["content"]}
                    for h in st.session_state.chat_history
                    if h["role"] == "user" or h["role"] == "assistant"
                ]

                answer, source_docs = get_rag_response(
                    client, user_query, indexed_data, embedding_model, history_for_rag
                )

                # Ge癟mii g羹ncelleme
                st.session_state.chat_history.append({"role": "user", "content": user_query})
                st.session_state.chat_history.append({"role": "assistant", "content": answer, "sources": source_docs})
                st.session_state.current_message = None

            except Exception as e:
                error_msg = f"zg羹n羹m, RAG zincirinde bir hata olutu: {e}"
                st.error(error_msg)
                st.session_state.chat_history.append({"role": "user", "content": user_query})
                st.session_state.chat_history.append({"role": "assistant", "content": error_msg})
                st.session_state.current_message = None

            st.rerun()